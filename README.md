# Framework de Pipeline ETL Dimensional com PySpark

Este reposit√≥rio apresenta um framework de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) avan√ßado, constru√≠do em Python e PySpark. O projeto foi refatorado para uma arquitetura orientada a objetos, tornando-o uma base reutiliz√°vel e configur√°vel para transformar dados transacionais brutos em um modelo dimensional (Star Schema) otimizado para consultas anal√≠ticas (OLAP).

## üéØ Objetivo do Projeto

O objetivo √© fornecer um exemplo completo e profissional de um pipeline de dados moderno, demonstrando solu√ß√µes para desafios comuns e implementando boas pr√°ticas de engenharia de dados, como:

  * **Reusabilidade e Manutenibilidade:** Criar um c√≥digo encapsulado em uma classe que seja f√°cil de manter, estender e adaptar para diferentes necessidades.
  * **Governan√ßa de Dados:** Implementar um framework de verifica√ß√£o de qualidade de dados (Data Quality) e gerar perfis de dados (Data Profiling) para garantir a confiabilidade e o monitoramento cont√≠nuo dos dados.
  * **Configura√ß√£o Desacoplada:** Separar a l√≥gica do pipeline de suas configura√ß√µes de ambiente, permitindo que ele seja executado em diferentes cen√°rios (desenvolvimento, produ√ß√£o) sem altera√ß√£o de c√≥digo.
  * **Modelagem Dimensional Extens√≠vel:** Construir um Star Schema e demonstrar como ele pode ser facilmente expandido com novas dimens√µes e fatos.

O projeto utiliza o dataset p√∫blico da **[Olist E-Commerce](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)** para simular um ambiente de dados transacionais real.

## ‚ú® Principais Funcionalidades

Este framework vai al√©m de um simples script de ETL, incorporando funcionalidades de n√≠vel de produ√ß√£o:

  * **Arquitetura Orientada a Objetos:** Toda a l√≥gica do pipeline √© encapsulada na classe `AdvancedStarSchemaPipeline`, promovendo organiza√ß√£o, reusabilidade e testabilidade.
  * **Configura√ß√£o via YAML:** Todas as configura√ß√µes, como caminhos, nomes e par√¢metros, s√£o gerenciadas em um arquivo `config.yaml` externo, permitindo f√°cil altera√ß√£o sem tocar no c√≥digo-fonte.
  * **Framework de Qualidade de Dados (DQ):**
      * **Verifica√ß√£o de Chaves:** Garante a unicidade e a n√£o nulidade das chaves prim√°rias nas dimens√µes.
      * **Integridade Referencial:** Valida se todas as chaves estrangeiras na tabela de fatos existem em suas respectivas dimens√µes (ex: `left_anti` join).
      * **Valores Aceitos:** Confere se colunas categ√≥ricas (como `order_status`) cont√™m apenas valores de uma lista predefinida.
      * **Valida√ß√£o de Intervalo:** Assegura que colunas num√©ricas (como `preco`) n√£o contenham valores inv√°lidos (ex: negativos).
  * **Perfil de Dados Automatizado (Data Profiling):** Gera um relat√≥rio em JSON com estat√≠sticas descritivas (m√©dia, desvio padr√£o, min, max, etc.) para colunas cr√≠ticas, auxiliando na detec√ß√£o de anomalias e data drift.
  * **Modelo Dimensional Extens√≠vel:** O design permite adicionar novas dimens√µes facilmente. O framework j√° inclui a **`Dim_Geolocalizacao`** como prova de conceito.
  * **Schemas Expl√≠citos e For√ßados:** Define e aplica schemas rigorosos na leitura dos dados de origem, prevenindo erros de tipo e garantindo a estabilidade do pipeline.

## ‚öôÔ∏è Tecnologias Utilizadas

  * **Python:** Linguagem principal para orquestra√ß√£o.
  * **Apache Spark (PySpark):** Motor de processamento de dados distribu√≠do.
  * **PyYAML:** Para carregar e gerenciar o arquivo de configura√ß√£o `config.yaml`.
  * **Requests:** Para download dos dados.

## üóÇÔ∏è Estrutura de Arquivos do Projeto

O pipeline assume e cria a seguinte estrutura de diret√≥rios para organizar os artefatos de dados:

```
.
‚îú‚îÄ‚îÄ config.yaml                 # Arquivo de configura√ß√£o principal
‚îú‚îÄ‚îÄ pipeline.py                 # O script do framework ETL
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ source/                 # Dados brutos baixados da Olist (.csv)
    ‚îú‚îÄ‚îÄ dimensional/            # Modelos dimensionais finais (.parquet)
    ‚îÇ   ‚îú‚îÄ‚îÄ Dim_Cliente/
    ‚îÇ   ‚îú‚îÄ‚îÄ Dim_Produto/
    ‚îÇ   ‚îú‚îÄ‚îÄ Dim_Vendedor/
    ‚îÇ   ‚îú‚îÄ‚îÄ Dim_Tempo/
    ‚îÇ   ‚îú‚îÄ‚îÄ Dim_Geolocalizacao/
    ‚îÇ   ‚îî‚îÄ‚îÄ Fato_Vendas/
    ‚îî‚îÄ‚îÄ profiling/
        ‚îî‚îÄ‚îÄ Fato_Vendas_profile.json # Relat√≥rio de perfil de dados
```

## üöÄ Como Executar o Pipeline

Siga os passos abaixo para configurar e rodar o projeto.

### 1\. Pr√©-requisitos

  * Python 3.9 ou superior.
  * Java Development Kit (JDK) 8 ou 11.
  * Clone o reposit√≥rio.
  * Instale as depend√™ncias Python:
    ```bash
    pip install pyspark requests pyyaml
    ```

### 2\. Crie o Arquivo de Configura√ß√£o

No diret√≥rio raiz do projeto, crie o arquivo **`config.yaml`** com o conte√∫do abaixo:

```yaml
# Configura√ß√µes Gerais do Pipeline
pipeline_name: "AdvancedStarSchemaPipeline"
log_level: "INFO"

# Configura√ß√µes do Spark
spark:
  master: "local[*]"
  app_name: "AdvancedStarSchemaPipeline"

# Configura√ß√µes de Dados
data:
  url: "https://github.com/g-bolota/olist-dataset/raw/main/olist_ecommerce_dataset.zip"
  source_path: "data/source"
  output_path: "data/dimensional"
  profiling_path: "data/profiling"

# Configura√ß√µes de Qualidade de Dados
data_quality:
  accepted_order_status:
    - "delivered"
    - "shipped"
    - "canceled"
    - "invoiced"
    - "processing"
    - "unavailable"
    - "approved"
    - "created"
```

### 3\. Execute o Pipeline

Com o arquivo `config.yaml` no lugar, execute o script principal:

```bash
python pipeline.py
```

O script ir√° automaticamente baixar os dados (na primeira execu√ß√£o), process√°-los, executar as valida√ß√µes, gerar o perfil e salvar os modelos dimensionais.

## üå† O Modelo Dimensional (Star Schema)

O pipeline constr√≥i o seguinte esquema estrela, otimizado para consultas anal√≠ticas:

  * **Tabela Fato:**
      * `Fato_Vendas`: Cont√©m as m√©tricas de neg√≥cio (`preco`, `valor_frete`) e as chaves para as dimens√µes.
  * **Tabelas de Dimens√£o:**
      * `Dim_Cliente`: Descreve os clientes (cidade, estado, geolocaliza√ß√£o).
      * `Dim_Produto`: Descreve os produtos (categoria, quantidade de fotos).
      * `Dim_Vendedor`: Descreve os vendedores (cidade, estado, geolocaliza√ß√£o).
      * `Dim_Tempo`: Descreve o tempo (ano, m√™s, dia, trimestre).
      * `Dim_Geolocalizacao`: Descreve a localiza√ß√£o geogr√°fica (latitude, longitude) a partir do CEP.

## üìà Melhorias Futuras

Este framework √© uma base s√≥lida que pode ser ainda mais aprimorada com:

  * **Orquestra√ß√£o de Workflows:** Integra√ß√£o com ferramentas como **Apache Airflow**, **Prefect** ou **Mage** para agendar e monitorar execu√ß√µes de forma robusta.
  * **Containeriza√ß√£o:** Empacotar a aplica√ß√£o com **Docker** para garantir a portabilidade e consist√™ncia entre diferentes ambientes.
  * **Testes Automatizados:** Implementa√ß√£o de testes unit√°rios e de integra√ß√£o com `pytest` para validar a l√≥gica de transforma√ß√£o de forma isolada.
  * **Cargas Incrementais e SCD:** Adicionar l√≥gica para lidar com cargas incrementais (processando apenas dados novos) e Slowly Changing Dimensions (SCD Tipo 2).

-----
